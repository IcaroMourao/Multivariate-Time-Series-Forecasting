{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d258821b37ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# imports required for the algorithm\n",
    "import os\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to load and save data\n",
    "data_input_path = '../data_in/CE_Crateus.csv'\n",
    "data_output_path = '../data_out/weather.csv'\n",
    "data_output_path_npsave = '../data_out/weather.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform date into appropriate format\n",
    "def parse(x):\n",
    "\treturn datetime.strptime(x, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input dataset and save like a output dataset\n",
    "def format_dataset(data_input_path, output_path):\n",
    "    try:\n",
    "        dataset = read_csv(data_input_path, parse_dates=['Data'], index_col=1, date_parser=parse, encoding='latin-1')\n",
    "    except:\n",
    "        dataset = read_csv(data_input_path, parse_dates=['Data'], index_col=1, date_parser=parse, encoding='latin-1', skiprows=2)\n",
    "    \n",
    "    dataset.index.name = 'date'\n",
    "\n",
    "    dataset.drop('Unnamed: 11', axis=1, inplace=True)\n",
    "    dataset.drop('Estacao', axis=1, inplace=True)\n",
    "    \n",
    "    dataset['TempMinima'] = dataset['TempMinima'].shift(-1)\n",
    "    dataset['Precipitacao'] = dataset['Precipitacao'].shift(-1)\n",
    "    \n",
    "    dataset.drop('Hora', axis=1, inplace=True)\n",
    "    \n",
    "    dataset = dataset.loc[~dataset.index.duplicated(keep='first')]\n",
    "    \n",
    "    # delete all NA values\n",
    "    dataset[:].fillna(0, inplace=True)\n",
    "\n",
    "    # show the first 5 lines on the dataset\n",
    "    print(dataset.head())\n",
    "        \n",
    "    # save file\n",
    "    dataset.to_csv(output_path)\n",
    "\n",
    "format_dataset(data_input_path, data_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def load_dataset(input_path):\n",
    "    return read_csv(input_path, header=0, index_col=0)\n",
    "\n",
    "dataset = load_dataset(data_output_path)\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each column in a graph\n",
    "def plot_columns(values):\n",
    "    groups = list(range(0,len(values[0])))\n",
    "    i = 1\n",
    "    \n",
    "    pyplot.figure(dpi=120)\n",
    "    for group in groups:\n",
    "        pyplot.subplot(len(groups), 1, i)\n",
    "        pyplot.plot(values[:, group])\n",
    "        pyplot.title(dataset.columns[group], y=0.1, loc='right')\n",
    "        i += 1\n",
    "    pyplot.show()\n",
    "\n",
    "plot_columns(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(dataset, day):\n",
    "    y_test = np.array([dataset.loc[day]['TempMaxima'], dataset.loc[day]['TempMinima']])\n",
    "    y_test = y_test.reshape(1,2)\n",
    "    \n",
    "    dataset = dataset[:-1]\n",
    "    \n",
    "    x_test = np.array(dataset[-60:])\n",
    "    x_test = x_test.reshape(1,60,8)\n",
    "    \n",
    "    lista = []\n",
    "    \n",
    "    for i in list(dataset.index):\n",
    "        lista.append([dataset.loc[i]['TempMaxima'], dataset.loc[i]['TempMinima']])\n",
    "    y_train = np.array(lista[60:])\n",
    "    \n",
    "    dataset = dataset[:-1]\n",
    "    \n",
    "    x_train = np.array(dataset[-60:])\n",
    "    x_train = x_train.reshape(1,60,8)\n",
    "    dataset = dataset[:-1]\n",
    "    \n",
    "    while len(dataset) >= 60:\n",
    "        array_aux = np.array(dataset[-60:])\n",
    "        array_aux = array_aux.reshape(1,60,8)\n",
    "        x_train = np.concatenate((x_train, array_aux), axis=0)\n",
    "        dataset = dataset[:-1]\n",
    "    \n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner loop\n",
    "def myLSTM(x_train, y_train):\n",
    "    x_val = x_train[-1]\n",
    "    y_val = y_train[-1]\n",
    "    x_val = x_val.reshape(1,60,8)\n",
    "    y_val = y_val.reshape(1,2)\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(Dense(16, activation='relu'))   # worth it?\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "    ]\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=1000, \n",
    "                        batch_size=72, \n",
    "                        validation_data=(x_val, y_val), \n",
    "                        verbose=2, \n",
    "                        shuffle=True, \n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    # plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "    # make a prediction\n",
    "    y_hat = model.predict(x_test)\n",
    "    \n",
    "    print('\\npredict: {}'.format(y_hat))\n",
    "    print('real:      {}\\n'.format(y_test))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "def metrics(model, x_test, y_test):\n",
    "    y_hat = model.predict(x_test)\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(y_test, y_hat))\n",
    "    variance = explained_variance_score(y_test, y_hat)\n",
    "    maxError = np.max(np.abs(y_test - y_hat))\n",
    "    meanAbsolutError = mean_absolute_error(y_test, y_hat)\n",
    "    medianAbsoluteError = np.median(np.abs(y_test - y_hat))\n",
    "    \n",
    "    return [rmse, variance, maxError, meanAbsolutError, medianAbsoluteError]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nested Cross Validation\n",
    "def NCV(dataset, days_to_predict):\n",
    "    metrics = []\n",
    "    rmse = []\n",
    "    variance = []\n",
    "    maxError = []\n",
    "    meanAbsolutError = []\n",
    "    medianAbsoluteError = []\n",
    "    i = len(days_to_predict) - 1\n",
    "    \n",
    "    # outer loop\n",
    "    while(i >= 0):\n",
    "        data = series_to_supervised(dataset, days_to_predict[i])\n",
    "        x_train = data[0]\n",
    "        y_train = data[1]\n",
    "        x_test = data[2]\n",
    "        y_test = data[3]\n",
    "\n",
    "        model = myLSTM(x_train, y_train)    \n",
    "        metrics += metrics(model, x_test, y_test)\n",
    "        \n",
    "        rmse += metrics[0]\n",
    "        variance += metrics[1]\n",
    "        maxError += metrics[2]\n",
    "        meanAbsolutError += metrics[3]\n",
    "        medianAbsoluteError += metrics[4]\n",
    "        \n",
    "        i -= 1\n",
    "        dataset = dataset[:-1]\n",
    "        \n",
    "    print('Test RMSE -------------------------- {:.4}'.format(np.mean(rmse)))\n",
    "    print('Test VARIANCE ---------------------- {:.4}'.format(np.mean(variance)))\n",
    "    print('Test MAX-ERROR --------------------- {:.4}'.format(np.mean(maxError)))\n",
    "    print('Test MEAN-ABSOLUTE-ERROR ----------- {:.4}'.format(np.mean(meanAbsolutError)))\n",
    "    print('Test MEDIAN-ABSOLUTE-ERROR --------- {:.4}'.format(np.mean(medianAbsoluteError)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to get index the last 30 days\n",
    "days_to_predict = list(dataset.index[-30:])\n",
    "\n",
    "dataset = dataset[:-1]\n",
    "\n",
    "NCV(dataset, days_to_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
